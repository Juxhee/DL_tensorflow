{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-4d33590375e7>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터 다운로드 \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 필요한 설정값들을 정의\n",
    "learning_rate = 0.02\n",
    "training_epochs = 50 # 반복횟수\n",
    "batch_size = 256 # 배치 개수 \n",
    "display_step = 1 # 손실 함수 출력 주기 \n",
    "examples_to_show = 10 #보여줄 MNIST Reconstruction 이미지 개수\n",
    "input_size = 784  # 28*28\n",
    "hidden1_size = 256\n",
    "hidden2_size = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder는 Unsupervised Learning 이므로 타겟 레이블(label) y 가 필요하지 않다 \n",
    "x = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "\n",
    "# Autoencoder 구조를 정의\n",
    "def build_autoencoder(x):\n",
    "    #인코딩(Encoding) - 784 -> 256 -> 128 \n",
    "    W1 = tf.Variable(tf.random_normal(shape=[input_size, hidden1_size]))\n",
    "    b1 = tf.Variable(tf.random_normal(shape=[hidden1_size]))\n",
    "    H1_output = tf.nn.sigmoid(tf.matmul(x,W1)+b1)\n",
    "    W2 = tf.Variable(tf.random_normal(shape=[hidden1_size, hidden2_size]))\n",
    "    b2 = tf.Variable(tf.random_normal(shape=[hidden2_size]))\n",
    "    H2_output = tf.nn.sigmoid(tf.matmul(H1_output,W2)+b2)\n",
    "    \n",
    "    #디코딩 - 128 -> 256 -> 784\n",
    "    W3 = tf.Variable(tf.random_normal(shape=[hidden2_size, hidden1_size]))\n",
    "    b3 = tf.Variable(tf.random_normal(shape=[hidden1_size]))\n",
    "    H3_output = tf.nn.sigmoid(tf.matmul(H2_output,W3)+b3)\n",
    "    W4 = tf.Variable(tf.random_normal(shape=[hidden1_size, input_size]))\n",
    "    b4 = tf.Variable(tf.random_normal(shape=[input_size]))\n",
    "    reconstructed_x= tf.nn.sigmoid(tf.matmul(H3_output,W4)+b4)\n",
    "    \n",
    "    return reconstructed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# autoencoder 선언\n",
    "y_pred = build_autoencoder(x)\n",
    "\n",
    "# 타겟 데이터는 인풋 데이터와 같다 \n",
    "y_true = x\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2)) # MSE(Mean of Squared Error) 손실함수 \n",
    "train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복(Epoch): 1, 손실함수(loss):0.212247\n",
      "반복(Epoch): 2, 손실함수(loss):0.150546\n",
      "반복(Epoch): 3, 손실함수(loss):0.131069\n",
      "반복(Epoch): 4, 손실함수(loss):0.120779\n",
      "반복(Epoch): 5, 손실함수(loss):0.111954\n",
      "반복(Epoch): 6, 손실함수(loss):0.107476\n",
      "반복(Epoch): 7, 손실함수(loss):0.095030\n",
      "반복(Epoch): 8, 손실함수(loss):0.095688\n",
      "반복(Epoch): 9, 손실함수(loss):0.087324\n",
      "반복(Epoch): 10, 손실함수(loss):0.087390\n",
      "반복(Epoch): 11, 손실함수(loss):0.086448\n",
      "반복(Epoch): 12, 손실함수(loss):0.085955\n",
      "반복(Epoch): 13, 손실함수(loss):0.082078\n",
      "반복(Epoch): 14, 손실함수(loss):0.080572\n",
      "반복(Epoch): 15, 손실함수(loss):0.074378\n",
      "반복(Epoch): 16, 손실함수(loss):0.073772\n",
      "반복(Epoch): 17, 손실함수(loss):0.070879\n",
      "반복(Epoch): 18, 손실함수(loss):0.072242\n",
      "반복(Epoch): 19, 손실함수(loss):0.068892\n",
      "반복(Epoch): 20, 손실함수(loss):0.070452\n",
      "반복(Epoch): 21, 손실함수(loss):0.069814\n",
      "반복(Epoch): 22, 손실함수(loss):0.069877\n",
      "반복(Epoch): 23, 손실함수(loss):0.068533\n",
      "반복(Epoch): 24, 손실함수(loss):0.067881\n",
      "반복(Epoch): 25, 손실함수(loss):0.069487\n",
      "반복(Epoch): 26, 손실함수(loss):0.069471\n",
      "반복(Epoch): 27, 손실함수(loss):0.068295\n",
      "반복(Epoch): 28, 손실함수(loss):0.066533\n",
      "반복(Epoch): 29, 손실함수(loss):0.068122\n",
      "반복(Epoch): 30, 손실함수(loss):0.065669\n",
      "반복(Epoch): 31, 손실함수(loss):0.064402\n",
      "반복(Epoch): 32, 손실함수(loss):0.065886\n",
      "반복(Epoch): 33, 손실함수(loss):0.063357\n",
      "반복(Epoch): 34, 손실함수(loss):0.062753\n",
      "반복(Epoch): 35, 손실함수(loss):0.060837\n",
      "반복(Epoch): 36, 손실함수(loss):0.061442\n",
      "반복(Epoch): 37, 손실함수(loss):0.059920\n",
      "반복(Epoch): 38, 손실함수(loss):0.061590\n",
      "반복(Epoch): 39, 손실함수(loss):0.060160\n",
      "반복(Epoch): 40, 손실함수(loss):0.058813\n",
      "반복(Epoch): 41, 손실함수(loss):0.059671\n",
      "반복(Epoch): 42, 손실함수(loss):0.059396\n",
      "반복(Epoch): 43, 손실함수(loss):0.057836\n",
      "반복(Epoch): 44, 손실함수(loss):0.059269\n",
      "반복(Epoch): 45, 손실함수(loss):0.059293\n",
      "반복(Epoch): 46, 손실함수(loss):0.057750\n",
      "반복(Epoch): 47, 손실함수(loss):0.058142\n",
      "반복(Epoch): 48, 손실함수(loss):0.054455\n",
      "반복(Epoch): 49, 손실함수(loss):0.056111\n",
      "반복(Epoch): 50, 손실함수(loss):0.051663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    }
   ],
   "source": [
    "# 세션을 열고 그래프 실행\n",
    "with tf.Session() as sess : \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs): \n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, current_loss = sess.run([train_step, loss],feed_dict={x: batch_xs})\n",
    "            \n",
    "        # 지정된 epoch마다 학습결과 출력\n",
    "        if epoch % display_step == 0 : \n",
    "            print(\"반복(Epoch): %d, 손실함수(loss):%f\" %((epoch+1),current_loss))\n",
    "    \n",
    "    reconstructed_result = sess.run(y_pred, feed_dict = {x: mnist.test.images\n",
    "[:examples_to_show]})\n",
    "    \n",
    "    f,a = plt.subplots(2,10,figsize=(10,2))\n",
    "    for i in range(examples_to_show): \n",
    "        a[0][i].imshow(np.reshape(mnist.test.images[i], (28,28)))\n",
    "        a[1][i].imshow(np.reshape(reconstructed_result[i],(28,28)))\n",
    "    f.savefig('reconstructed_mnist_image.png')\n",
    "    \n",
    "    f.show()\n",
    "    plt.draw()\n",
    "    plt.waitforbuttonpress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 필요한 설정값들을 정의\n",
    "learning_rate_RMSProp = 0.02\n",
    "learning_rate_GradientDescent = 0.5\n",
    "num_epochs = 100 # 반복횟수\n",
    "batch_size = 256 # 배치 개수 \n",
    "display_step = 1 # 손실 함수 출력 주기 \n",
    "input_size = 784  # 28*28\n",
    "hidden1_size = 128\n",
    "hidden2_size = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, input_size]) # 인풋을 위한 플레이스 홀더 정의 \n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) # True MNIST 숫자값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder 구조 정의 \n",
    "def build_autoencoder(x):\n",
    "    #인코딩(Encoding) - 784 -> 128 -> 64\n",
    "    Wh_1 = tf.Variable(tf.random_normal(shape=[input_size, hidden1_size]))\n",
    "    bh_1 = tf.Variable(tf.random_normal(shape=[hidden1_size]))\n",
    "    H1_output = tf.nn.sigmoid(tf.matmul(x,Wh_1)+bh_1)\n",
    "    Wh_2 = tf.Variable(tf.random_normal(shape=[hidden1_size, hidden2_size]))\n",
    "    bh_2 = tf.Variable(tf.random_normal(shape=[hidden2_size]))\n",
    "    H2_output = tf.nn.sigmoid(tf.matmul(H1_output,Wh_2)+bh_2)\n",
    "    \n",
    "    #디코딩 - 128 -> 256 -> 784 \n",
    "    Wh_3 = tf.Variable(tf.random_normal(shape=[hidden2_size, hidden1_size]))\n",
    "    bh_3 = tf.Variable(tf.random_normal(shape=[hidden1_size]))\n",
    "    H3_output = tf.nn.sigmoid(tf.matmul(H2_output,Wh_3)+bh_3)\n",
    "    Wo = tf.Variable(tf.random_normal(shape=[hidden1_size, input_size]))\n",
    "    bo = tf.Variable(tf.random_normal(shape=[input_size]))\n",
    "    X_reconstructed = tf.nn.sigmoid(tf.matmul(H3_output,Wo)+bo)\n",
    "    \n",
    "    return X_reconstructed, H2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 분류기 정의\n",
    "def build_softmax_classifier(x):\n",
    "    W_softmax = tf.Variable(tf.zeros([hidden2_size, 10]))\n",
    "    b_softmax = tf.Variable(tf.zeros([10]))\n",
    "    y_pred = tf.nn.softmax(tf.matmul(x,W_softmax) + b_softmax)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder 선언\n",
    "y_pred, extracted_features = build_autoencoder(x)\n",
    "\n",
    "# 타겟 데이터는 인풋 데이터와 같음 \n",
    "y_true = x \n",
    "\n",
    "# Softmax 분류기 선언(입력으로 Autoencoder의 압축된 특징을 넣는다)\n",
    "y_pred_softmax = build_softmax_classifier(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 1. Pre-Training : MNIST 데이터 재구축을 목적으로 하는 손실 함수와 옵티마이저를 정의\n",
    "pretraining_loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2)) # MSE 손실 함수 \n",
    "pretraining_train_step = tf.train.RMSPropOptimizer(learning_rate_RMSProp).minimize(pretraining_loss)\n",
    "\n",
    "# 2. Fine-Tuning : MNIST 데이터 분류를 목적으로 하는 손실 함수와 옵티마이저 정의 \n",
    "finetuning_loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_pred_softmax),reduction_indices=[1])) # cross-entropy loss 함수\n",
    "finetuning_train_step = tf.train.GradientDescentOptimizer(learning_rate_GradientDescent).minimize(finetuning_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복(Epoch): 1, Pre-training 손실 함수(pretraining_loss): 0.158409\n",
      "반복(Epoch): 2, Pre-training 손실 함수(pretraining_loss): 0.102113\n",
      "반복(Epoch): 3, Pre-training 손실 함수(pretraining_loss): 0.083326\n",
      "반복(Epoch): 4, Pre-training 손실 함수(pretraining_loss): 0.073570\n",
      "반복(Epoch): 5, Pre-training 손실 함수(pretraining_loss): 0.071608\n",
      "반복(Epoch): 6, Pre-training 손실 함수(pretraining_loss): 0.068544\n",
      "반복(Epoch): 7, Pre-training 손실 함수(pretraining_loss): 0.062290\n",
      "반복(Epoch): 8, Pre-training 손실 함수(pretraining_loss): 0.061349\n",
      "반복(Epoch): 9, Pre-training 손실 함수(pretraining_loss): 0.057887\n",
      "반복(Epoch): 10, Pre-training 손실 함수(pretraining_loss): 0.057767\n",
      "반복(Epoch): 11, Pre-training 손실 함수(pretraining_loss): 0.053408\n",
      "반복(Epoch): 12, Pre-training 손실 함수(pretraining_loss): 0.053751\n",
      "반복(Epoch): 13, Pre-training 손실 함수(pretraining_loss): 0.048839\n",
      "반복(Epoch): 14, Pre-training 손실 함수(pretraining_loss): 0.048625\n",
      "반복(Epoch): 15, Pre-training 손실 함수(pretraining_loss): 0.046656\n",
      "반복(Epoch): 16, Pre-training 손실 함수(pretraining_loss): 0.046301\n",
      "반복(Epoch): 17, Pre-training 손실 함수(pretraining_loss): 0.047261\n",
      "반복(Epoch): 18, Pre-training 손실 함수(pretraining_loss): 0.040387\n",
      "반복(Epoch): 19, Pre-training 손실 함수(pretraining_loss): 0.040462\n",
      "반복(Epoch): 20, Pre-training 손실 함수(pretraining_loss): 0.038735\n",
      "반복(Epoch): 21, Pre-training 손실 함수(pretraining_loss): 0.039044\n",
      "반복(Epoch): 22, Pre-training 손실 함수(pretraining_loss): 0.038523\n",
      "반복(Epoch): 23, Pre-training 손실 함수(pretraining_loss): 0.034043\n",
      "반복(Epoch): 24, Pre-training 손실 함수(pretraining_loss): 0.037587\n",
      "반복(Epoch): 25, Pre-training 손실 함수(pretraining_loss): 0.037866\n",
      "반복(Epoch): 26, Pre-training 손실 함수(pretraining_loss): 0.035032\n",
      "반복(Epoch): 27, Pre-training 손실 함수(pretraining_loss): 0.031777\n",
      "반복(Epoch): 28, Pre-training 손실 함수(pretraining_loss): 0.034479\n",
      "반복(Epoch): 29, Pre-training 손실 함수(pretraining_loss): 0.034534\n",
      "반복(Epoch): 30, Pre-training 손실 함수(pretraining_loss): 0.032038\n",
      "반복(Epoch): 31, Pre-training 손실 함수(pretraining_loss): 0.032709\n",
      "반복(Epoch): 32, Pre-training 손실 함수(pretraining_loss): 0.029885\n",
      "반복(Epoch): 33, Pre-training 손실 함수(pretraining_loss): 0.030097\n",
      "반복(Epoch): 34, Pre-training 손실 함수(pretraining_loss): 0.030100\n",
      "반복(Epoch): 35, Pre-training 손실 함수(pretraining_loss): 0.032856\n",
      "반복(Epoch): 36, Pre-training 손실 함수(pretraining_loss): 0.029984\n",
      "반복(Epoch): 37, Pre-training 손실 함수(pretraining_loss): 0.028950\n",
      "반복(Epoch): 38, Pre-training 손실 함수(pretraining_loss): 0.028086\n",
      "반복(Epoch): 39, Pre-training 손실 함수(pretraining_loss): 0.028971\n",
      "반복(Epoch): 40, Pre-training 손실 함수(pretraining_loss): 0.029509\n",
      "반복(Epoch): 41, Pre-training 손실 함수(pretraining_loss): 0.028185\n",
      "반복(Epoch): 42, Pre-training 손실 함수(pretraining_loss): 0.028000\n",
      "반복(Epoch): 43, Pre-training 손실 함수(pretraining_loss): 0.030780\n",
      "반복(Epoch): 44, Pre-training 손실 함수(pretraining_loss): 0.026668\n",
      "반복(Epoch): 45, Pre-training 손실 함수(pretraining_loss): 0.029453\n",
      "반복(Epoch): 46, Pre-training 손실 함수(pretraining_loss): 0.027575\n",
      "반복(Epoch): 47, Pre-training 손실 함수(pretraining_loss): 0.027257\n",
      "반복(Epoch): 48, Pre-training 손실 함수(pretraining_loss): 0.025617\n",
      "반복(Epoch): 49, Pre-training 손실 함수(pretraining_loss): 0.023385\n",
      "반복(Epoch): 50, Pre-training 손실 함수(pretraining_loss): 0.025108\n",
      "반복(Epoch): 51, Pre-training 손실 함수(pretraining_loss): 0.023719\n",
      "반복(Epoch): 52, Pre-training 손실 함수(pretraining_loss): 0.023106\n",
      "반복(Epoch): 53, Pre-training 손실 함수(pretraining_loss): 0.026286\n",
      "반복(Epoch): 54, Pre-training 손실 함수(pretraining_loss): 0.022813\n",
      "반복(Epoch): 55, Pre-training 손실 함수(pretraining_loss): 0.023689\n",
      "반복(Epoch): 56, Pre-training 손실 함수(pretraining_loss): 0.022219\n",
      "반복(Epoch): 57, Pre-training 손실 함수(pretraining_loss): 0.025285\n",
      "반복(Epoch): 58, Pre-training 손실 함수(pretraining_loss): 0.021946\n",
      "반복(Epoch): 59, Pre-training 손실 함수(pretraining_loss): 0.024273\n",
      "반복(Epoch): 60, Pre-training 손실 함수(pretraining_loss): 0.023142\n",
      "반복(Epoch): 61, Pre-training 손실 함수(pretraining_loss): 0.021720\n",
      "반복(Epoch): 62, Pre-training 손실 함수(pretraining_loss): 0.023895\n",
      "반복(Epoch): 63, Pre-training 손실 함수(pretraining_loss): 0.021706\n",
      "반복(Epoch): 64, Pre-training 손실 함수(pretraining_loss): 0.021306\n",
      "반복(Epoch): 65, Pre-training 손실 함수(pretraining_loss): 0.025090\n",
      "반복(Epoch): 66, Pre-training 손실 함수(pretraining_loss): 0.020961\n",
      "반복(Epoch): 67, Pre-training 손실 함수(pretraining_loss): 0.021329\n",
      "반복(Epoch): 68, Pre-training 손실 함수(pretraining_loss): 0.021758\n",
      "반복(Epoch): 69, Pre-training 손실 함수(pretraining_loss): 0.021681\n",
      "반복(Epoch): 70, Pre-training 손실 함수(pretraining_loss): 0.020448\n",
      "반복(Epoch): 71, Pre-training 손실 함수(pretraining_loss): 0.022665\n",
      "반복(Epoch): 72, Pre-training 손실 함수(pretraining_loss): 0.020854\n",
      "반복(Epoch): 73, Pre-training 손실 함수(pretraining_loss): 0.021650\n",
      "반복(Epoch): 74, Pre-training 손실 함수(pretraining_loss): 0.023095\n",
      "반복(Epoch): 75, Pre-training 손실 함수(pretraining_loss): 0.021679\n",
      "반복(Epoch): 76, Pre-training 손실 함수(pretraining_loss): 0.019574\n",
      "반복(Epoch): 77, Pre-training 손실 함수(pretraining_loss): 0.021493\n",
      "반복(Epoch): 78, Pre-training 손실 함수(pretraining_loss): 0.022163\n",
      "반복(Epoch): 79, Pre-training 손실 함수(pretraining_loss): 0.019832\n",
      "반복(Epoch): 80, Pre-training 손실 함수(pretraining_loss): 0.019779\n",
      "반복(Epoch): 81, Pre-training 손실 함수(pretraining_loss): 0.020138\n",
      "반복(Epoch): 82, Pre-training 손실 함수(pretraining_loss): 0.020327\n",
      "반복(Epoch): 83, Pre-training 손실 함수(pretraining_loss): 0.021548\n",
      "반복(Epoch): 84, Pre-training 손실 함수(pretraining_loss): 0.021290\n",
      "반복(Epoch): 85, Pre-training 손실 함수(pretraining_loss): 0.019595\n",
      "반복(Epoch): 86, Pre-training 손실 함수(pretraining_loss): 0.019612\n",
      "반복(Epoch): 87, Pre-training 손실 함수(pretraining_loss): 0.019921\n",
      "반복(Epoch): 88, Pre-training 손실 함수(pretraining_loss): 0.021861\n",
      "반복(Epoch): 89, Pre-training 손실 함수(pretraining_loss): 0.019015\n",
      "반복(Epoch): 90, Pre-training 손실 함수(pretraining_loss): 0.021398\n",
      "반복(Epoch): 91, Pre-training 손실 함수(pretraining_loss): 0.021279\n",
      "반복(Epoch): 92, Pre-training 손실 함수(pretraining_loss): 0.020215\n",
      "반복(Epoch): 93, Pre-training 손실 함수(pretraining_loss): 0.019797\n",
      "반복(Epoch): 94, Pre-training 손실 함수(pretraining_loss): 0.018378\n",
      "반복(Epoch): 95, Pre-training 손실 함수(pretraining_loss): 0.019294\n",
      "반복(Epoch): 96, Pre-training 손실 함수(pretraining_loss): 0.020829\n",
      "반복(Epoch): 97, Pre-training 손실 함수(pretraining_loss): 0.018583\n",
      "반복(Epoch): 98, Pre-training 손실 함수(pretraining_loss): 0.019103\n",
      "반복(Epoch): 99, Pre-training 손실 함수(pretraining_loss): 0.020169\n",
      "반복(Epoch): 100, Pre-training 손실 함수(pretraining_loss): 0.018819\n",
      "Step 1 : MNIST 데이터 재구축을 위한 오토인코더 최적화 완료\n",
      "반복(Epoch): 1, Fine-tuning 손실 함수(finetuning_loss): 0.497003\n",
      "반복(Epoch): 2, Fine-tuning 손실 함수(finetuning_loss): 0.424076\n",
      "반복(Epoch): 3, Fine-tuning 손실 함수(finetuning_loss): 0.327665\n",
      "반복(Epoch): 4, Fine-tuning 손실 함수(finetuning_loss): 0.288097\n",
      "반복(Epoch): 5, Fine-tuning 손실 함수(finetuning_loss): 0.245304\n",
      "반복(Epoch): 6, Fine-tuning 손실 함수(finetuning_loss): 0.268169\n",
      "반복(Epoch): 7, Fine-tuning 손실 함수(finetuning_loss): 0.204784\n",
      "반복(Epoch): 8, Fine-tuning 손실 함수(finetuning_loss): 0.168902\n",
      "반복(Epoch): 9, Fine-tuning 손실 함수(finetuning_loss): 0.215572\n",
      "반복(Epoch): 10, Fine-tuning 손실 함수(finetuning_loss): 0.228033\n",
      "반복(Epoch): 11, Fine-tuning 손실 함수(finetuning_loss): 0.288686\n",
      "반복(Epoch): 12, Fine-tuning 손실 함수(finetuning_loss): 0.144642\n",
      "반복(Epoch): 13, Fine-tuning 손실 함수(finetuning_loss): 0.214938\n",
      "반복(Epoch): 14, Fine-tuning 손실 함수(finetuning_loss): 0.195424\n",
      "반복(Epoch): 15, Fine-tuning 손실 함수(finetuning_loss): 0.185348\n",
      "반복(Epoch): 16, Fine-tuning 손실 함수(finetuning_loss): 0.158786\n",
      "반복(Epoch): 17, Fine-tuning 손실 함수(finetuning_loss): 0.185693\n",
      "반복(Epoch): 18, Fine-tuning 손실 함수(finetuning_loss): 0.117124\n",
      "반복(Epoch): 19, Fine-tuning 손실 함수(finetuning_loss): 0.127578\n",
      "반복(Epoch): 20, Fine-tuning 손실 함수(finetuning_loss): 0.188503\n",
      "반복(Epoch): 21, Fine-tuning 손실 함수(finetuning_loss): 0.084191\n",
      "반복(Epoch): 22, Fine-tuning 손실 함수(finetuning_loss): 0.134266\n",
      "반복(Epoch): 23, Fine-tuning 손실 함수(finetuning_loss): 0.124254\n",
      "반복(Epoch): 24, Fine-tuning 손실 함수(finetuning_loss): 0.124712\n",
      "반복(Epoch): 25, Fine-tuning 손실 함수(finetuning_loss): 0.154494\n",
      "반복(Epoch): 26, Fine-tuning 손실 함수(finetuning_loss): 0.114421\n",
      "반복(Epoch): 27, Fine-tuning 손실 함수(finetuning_loss): 0.144693\n",
      "반복(Epoch): 28, Fine-tuning 손실 함수(finetuning_loss): 0.095697\n",
      "반복(Epoch): 29, Fine-tuning 손실 함수(finetuning_loss): 0.116161\n",
      "반복(Epoch): 30, Fine-tuning 손실 함수(finetuning_loss): 0.100487\n",
      "반복(Epoch): 31, Fine-tuning 손실 함수(finetuning_loss): 0.101898\n",
      "반복(Epoch): 32, Fine-tuning 손실 함수(finetuning_loss): 0.080598\n",
      "반복(Epoch): 33, Fine-tuning 손실 함수(finetuning_loss): 0.109240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복(Epoch): 34, Fine-tuning 손실 함수(finetuning_loss): 0.116683\n",
      "반복(Epoch): 35, Fine-tuning 손실 함수(finetuning_loss): 0.070720\n",
      "반복(Epoch): 36, Fine-tuning 손실 함수(finetuning_loss): 0.121867\n",
      "반복(Epoch): 37, Fine-tuning 손실 함수(finetuning_loss): 0.123734\n",
      "반복(Epoch): 38, Fine-tuning 손실 함수(finetuning_loss): 0.088605\n",
      "반복(Epoch): 39, Fine-tuning 손실 함수(finetuning_loss): 0.110419\n",
      "반복(Epoch): 40, Fine-tuning 손실 함수(finetuning_loss): 0.112773\n",
      "반복(Epoch): 41, Fine-tuning 손실 함수(finetuning_loss): 0.094069\n",
      "반복(Epoch): 42, Fine-tuning 손실 함수(finetuning_loss): 0.074302\n",
      "반복(Epoch): 43, Fine-tuning 손실 함수(finetuning_loss): 0.110518\n",
      "반복(Epoch): 44, Fine-tuning 손실 함수(finetuning_loss): 0.058731\n",
      "반복(Epoch): 45, Fine-tuning 손실 함수(finetuning_loss): 0.093832\n",
      "반복(Epoch): 46, Fine-tuning 손실 함수(finetuning_loss): 0.059545\n",
      "반복(Epoch): 47, Fine-tuning 손실 함수(finetuning_loss): 0.167633\n",
      "반복(Epoch): 48, Fine-tuning 손실 함수(finetuning_loss): 0.072743\n",
      "반복(Epoch): 49, Fine-tuning 손실 함수(finetuning_loss): 0.142617\n",
      "반복(Epoch): 50, Fine-tuning 손실 함수(finetuning_loss): 0.082484\n",
      "반복(Epoch): 51, Fine-tuning 손실 함수(finetuning_loss): 0.142174\n",
      "반복(Epoch): 52, Fine-tuning 손실 함수(finetuning_loss): 0.058738\n",
      "반복(Epoch): 53, Fine-tuning 손실 함수(finetuning_loss): 0.068713\n",
      "반복(Epoch): 54, Fine-tuning 손실 함수(finetuning_loss): 0.086708\n",
      "반복(Epoch): 55, Fine-tuning 손실 함수(finetuning_loss): 0.107451\n",
      "반복(Epoch): 56, Fine-tuning 손실 함수(finetuning_loss): 0.101336\n",
      "반복(Epoch): 57, Fine-tuning 손실 함수(finetuning_loss): 0.069484\n",
      "반복(Epoch): 58, Fine-tuning 손실 함수(finetuning_loss): 0.115532\n",
      "반복(Epoch): 59, Fine-tuning 손실 함수(finetuning_loss): 0.058662\n",
      "반복(Epoch): 60, Fine-tuning 손실 함수(finetuning_loss): 0.064989\n",
      "반복(Epoch): 61, Fine-tuning 손실 함수(finetuning_loss): 0.097942\n",
      "반복(Epoch): 62, Fine-tuning 손실 함수(finetuning_loss): 0.054946\n",
      "반복(Epoch): 63, Fine-tuning 손실 함수(finetuning_loss): 0.060975\n",
      "반복(Epoch): 64, Fine-tuning 손실 함수(finetuning_loss): 0.053156\n",
      "반복(Epoch): 65, Fine-tuning 손실 함수(finetuning_loss): 0.098924\n",
      "반복(Epoch): 66, Fine-tuning 손실 함수(finetuning_loss): 0.111200\n",
      "반복(Epoch): 67, Fine-tuning 손실 함수(finetuning_loss): 0.051736\n",
      "반복(Epoch): 68, Fine-tuning 손실 함수(finetuning_loss): 0.072874\n",
      "반복(Epoch): 69, Fine-tuning 손실 함수(finetuning_loss): 0.049212\n",
      "반복(Epoch): 70, Fine-tuning 손실 함수(finetuning_loss): 0.054141\n",
      "반복(Epoch): 71, Fine-tuning 손실 함수(finetuning_loss): 0.087486\n",
      "반복(Epoch): 72, Fine-tuning 손실 함수(finetuning_loss): 0.039141\n",
      "반복(Epoch): 73, Fine-tuning 손실 함수(finetuning_loss): 0.080944\n",
      "반복(Epoch): 74, Fine-tuning 손실 함수(finetuning_loss): 0.055961\n",
      "반복(Epoch): 75, Fine-tuning 손실 함수(finetuning_loss): 0.037787\n",
      "반복(Epoch): 76, Fine-tuning 손실 함수(finetuning_loss): 0.076385\n",
      "반복(Epoch): 77, Fine-tuning 손실 함수(finetuning_loss): 0.080981\n",
      "반복(Epoch): 78, Fine-tuning 손실 함수(finetuning_loss): 0.042956\n",
      "반복(Epoch): 79, Fine-tuning 손실 함수(finetuning_loss): 0.074515\n",
      "반복(Epoch): 80, Fine-tuning 손실 함수(finetuning_loss): 0.047370\n",
      "반복(Epoch): 81, Fine-tuning 손실 함수(finetuning_loss): 0.047199\n",
      "반복(Epoch): 82, Fine-tuning 손실 함수(finetuning_loss): 0.040879\n",
      "반복(Epoch): 83, Fine-tuning 손실 함수(finetuning_loss): 0.052145\n",
      "반복(Epoch): 84, Fine-tuning 손실 함수(finetuning_loss): 0.049538\n",
      "반복(Epoch): 85, Fine-tuning 손실 함수(finetuning_loss): 0.095997\n",
      "반복(Epoch): 86, Fine-tuning 손실 함수(finetuning_loss): 0.030882\n",
      "반복(Epoch): 87, Fine-tuning 손실 함수(finetuning_loss): 0.057391\n",
      "반복(Epoch): 88, Fine-tuning 손실 함수(finetuning_loss): 0.032579\n",
      "반복(Epoch): 89, Fine-tuning 손실 함수(finetuning_loss): 0.043976\n",
      "반복(Epoch): 90, Fine-tuning 손실 함수(finetuning_loss): 0.040168\n",
      "반복(Epoch): 91, Fine-tuning 손실 함수(finetuning_loss): 0.033727\n",
      "반복(Epoch): 92, Fine-tuning 손실 함수(finetuning_loss): 0.059062\n",
      "반복(Epoch): 93, Fine-tuning 손실 함수(finetuning_loss): 0.058358\n",
      "반복(Epoch): 94, Fine-tuning 손실 함수(finetuning_loss): 0.025272\n",
      "반복(Epoch): 95, Fine-tuning 손실 함수(finetuning_loss): 0.047981\n",
      "반복(Epoch): 96, Fine-tuning 손실 함수(finetuning_loss): 0.044740\n",
      "반복(Epoch): 97, Fine-tuning 손실 함수(finetuning_loss): 0.032833\n",
      "반복(Epoch): 98, Fine-tuning 손실 함수(finetuning_loss): 0.040719\n",
      "반복(Epoch): 99, Fine-tuning 손실 함수(finetuning_loss): 0.032946\n",
      "반복(Epoch): 100, Fine-tuning 손실 함수(finetuning_loss): 0.023736\n",
      "반복(Epoch): 101, Fine-tuning 손실 함수(finetuning_loss): 0.030392\n",
      "반복(Epoch): 102, Fine-tuning 손실 함수(finetuning_loss): 0.032289\n",
      "반복(Epoch): 103, Fine-tuning 손실 함수(finetuning_loss): 0.054658\n",
      "반복(Epoch): 104, Fine-tuning 손실 함수(finetuning_loss): 0.045350\n",
      "반복(Epoch): 105, Fine-tuning 손실 함수(finetuning_loss): 0.029482\n",
      "반복(Epoch): 106, Fine-tuning 손실 함수(finetuning_loss): 0.032664\n",
      "반복(Epoch): 107, Fine-tuning 손실 함수(finetuning_loss): 0.064726\n",
      "반복(Epoch): 108, Fine-tuning 손실 함수(finetuning_loss): 0.038432\n",
      "반복(Epoch): 109, Fine-tuning 손실 함수(finetuning_loss): 0.037263\n",
      "반복(Epoch): 110, Fine-tuning 손실 함수(finetuning_loss): 0.023301\n",
      "반복(Epoch): 111, Fine-tuning 손실 함수(finetuning_loss): 0.049685\n",
      "반복(Epoch): 112, Fine-tuning 손실 함수(finetuning_loss): 0.027573\n",
      "반복(Epoch): 113, Fine-tuning 손실 함수(finetuning_loss): 0.024442\n",
      "반복(Epoch): 114, Fine-tuning 손실 함수(finetuning_loss): 0.018260\n",
      "반복(Epoch): 115, Fine-tuning 손실 함수(finetuning_loss): 0.021817\n",
      "반복(Epoch): 116, Fine-tuning 손실 함수(finetuning_loss): 0.023084\n",
      "반복(Epoch): 117, Fine-tuning 손실 함수(finetuning_loss): 0.071123\n",
      "반복(Epoch): 118, Fine-tuning 손실 함수(finetuning_loss): 0.053362\n",
      "반복(Epoch): 119, Fine-tuning 손실 함수(finetuning_loss): 0.027038\n",
      "반복(Epoch): 120, Fine-tuning 손실 함수(finetuning_loss): 0.025392\n",
      "반복(Epoch): 121, Fine-tuning 손실 함수(finetuning_loss): 0.031087\n",
      "반복(Epoch): 122, Fine-tuning 손실 함수(finetuning_loss): 0.046823\n",
      "반복(Epoch): 123, Fine-tuning 손실 함수(finetuning_loss): 0.032939\n",
      "반복(Epoch): 124, Fine-tuning 손실 함수(finetuning_loss): 0.025365\n",
      "반복(Epoch): 125, Fine-tuning 손실 함수(finetuning_loss): 0.029055\n",
      "반복(Epoch): 126, Fine-tuning 손실 함수(finetuning_loss): 0.042178\n",
      "반복(Epoch): 127, Fine-tuning 손실 함수(finetuning_loss): 0.021366\n",
      "반복(Epoch): 128, Fine-tuning 손실 함수(finetuning_loss): 0.028341\n",
      "반복(Epoch): 129, Fine-tuning 손실 함수(finetuning_loss): 0.033887\n",
      "반복(Epoch): 130, Fine-tuning 손실 함수(finetuning_loss): 0.016745\n",
      "반복(Epoch): 131, Fine-tuning 손실 함수(finetuning_loss): 0.016706\n",
      "반복(Epoch): 132, Fine-tuning 손실 함수(finetuning_loss): 0.011891\n",
      "반복(Epoch): 133, Fine-tuning 손실 함수(finetuning_loss): 0.033335\n",
      "반복(Epoch): 134, Fine-tuning 손실 함수(finetuning_loss): 0.023724\n",
      "반복(Epoch): 135, Fine-tuning 손실 함수(finetuning_loss): 0.045809\n",
      "반복(Epoch): 136, Fine-tuning 손실 함수(finetuning_loss): 0.008870\n",
      "반복(Epoch): 137, Fine-tuning 손실 함수(finetuning_loss): 0.020092\n",
      "반복(Epoch): 138, Fine-tuning 손실 함수(finetuning_loss): 0.023990\n",
      "반복(Epoch): 139, Fine-tuning 손실 함수(finetuning_loss): 0.022002\n",
      "반복(Epoch): 140, Fine-tuning 손실 함수(finetuning_loss): 0.016955\n",
      "반복(Epoch): 141, Fine-tuning 손실 함수(finetuning_loss): 0.011494\n",
      "반복(Epoch): 142, Fine-tuning 손실 함수(finetuning_loss): 0.027350\n",
      "반복(Epoch): 143, Fine-tuning 손실 함수(finetuning_loss): 0.016854\n",
      "반복(Epoch): 144, Fine-tuning 손실 함수(finetuning_loss): 0.046541\n",
      "반복(Epoch): 145, Fine-tuning 손실 함수(finetuning_loss): 0.029398\n",
      "반복(Epoch): 146, Fine-tuning 손실 함수(finetuning_loss): 0.028495\n",
      "반복(Epoch): 147, Fine-tuning 손실 함수(finetuning_loss): 0.015600\n",
      "반복(Epoch): 148, Fine-tuning 손실 함수(finetuning_loss): 0.029664\n",
      "반복(Epoch): 149, Fine-tuning 손실 함수(finetuning_loss): 0.023084\n",
      "반복(Epoch): 150, Fine-tuning 손실 함수(finetuning_loss): 0.016123\n",
      "반복(Epoch): 151, Fine-tuning 손실 함수(finetuning_loss): 0.012918\n",
      "반복(Epoch): 152, Fine-tuning 손실 함수(finetuning_loss): 0.043216\n",
      "반복(Epoch): 153, Fine-tuning 손실 함수(finetuning_loss): 0.030550\n",
      "반복(Epoch): 154, Fine-tuning 손실 함수(finetuning_loss): 0.020602\n",
      "반복(Epoch): 155, Fine-tuning 손실 함수(finetuning_loss): 0.014286\n",
      "반복(Epoch): 156, Fine-tuning 손실 함수(finetuning_loss): 0.074025\n",
      "반복(Epoch): 157, Fine-tuning 손실 함수(finetuning_loss): 0.020402\n",
      "반복(Epoch): 158, Fine-tuning 손실 함수(finetuning_loss): 0.023300\n",
      "반복(Epoch): 159, Fine-tuning 손실 함수(finetuning_loss): 0.015916\n",
      "반복(Epoch): 160, Fine-tuning 손실 함수(finetuning_loss): 0.022884\n",
      "반복(Epoch): 161, Fine-tuning 손실 함수(finetuning_loss): 0.019942\n",
      "반복(Epoch): 162, Fine-tuning 손실 함수(finetuning_loss): 0.021130\n",
      "반복(Epoch): 163, Fine-tuning 손실 함수(finetuning_loss): 0.027647\n",
      "반복(Epoch): 164, Fine-tuning 손실 함수(finetuning_loss): 0.016309\n",
      "반복(Epoch): 165, Fine-tuning 손실 함수(finetuning_loss): 0.014787\n",
      "반복(Epoch): 166, Fine-tuning 손실 함수(finetuning_loss): 0.017645\n",
      "반복(Epoch): 167, Fine-tuning 손실 함수(finetuning_loss): 0.013863\n",
      "반복(Epoch): 168, Fine-tuning 손실 함수(finetuning_loss): 0.018868\n",
      "반복(Epoch): 169, Fine-tuning 손실 함수(finetuning_loss): 0.030196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복(Epoch): 170, Fine-tuning 손실 함수(finetuning_loss): 0.012589\n",
      "반복(Epoch): 171, Fine-tuning 손실 함수(finetuning_loss): 0.068301\n",
      "반복(Epoch): 172, Fine-tuning 손실 함수(finetuning_loss): 0.039472\n",
      "반복(Epoch): 173, Fine-tuning 손실 함수(finetuning_loss): 0.008274\n",
      "반복(Epoch): 174, Fine-tuning 손실 함수(finetuning_loss): 0.031166\n",
      "반복(Epoch): 175, Fine-tuning 손실 함수(finetuning_loss): 0.008359\n",
      "반복(Epoch): 176, Fine-tuning 손실 함수(finetuning_loss): 0.019494\n",
      "반복(Epoch): 177, Fine-tuning 손실 함수(finetuning_loss): 0.011144\n",
      "반복(Epoch): 178, Fine-tuning 손실 함수(finetuning_loss): 0.011454\n",
      "반복(Epoch): 179, Fine-tuning 손실 함수(finetuning_loss): 0.015236\n",
      "반복(Epoch): 180, Fine-tuning 손실 함수(finetuning_loss): 0.012255\n",
      "반복(Epoch): 181, Fine-tuning 손실 함수(finetuning_loss): 0.010164\n",
      "반복(Epoch): 182, Fine-tuning 손실 함수(finetuning_loss): 0.018434\n",
      "반복(Epoch): 183, Fine-tuning 손실 함수(finetuning_loss): 0.009811\n",
      "반복(Epoch): 184, Fine-tuning 손실 함수(finetuning_loss): 0.017262\n",
      "반복(Epoch): 185, Fine-tuning 손실 함수(finetuning_loss): 0.062950\n",
      "반복(Epoch): 186, Fine-tuning 손실 함수(finetuning_loss): 0.009838\n",
      "반복(Epoch): 187, Fine-tuning 손실 함수(finetuning_loss): 0.024425\n",
      "반복(Epoch): 188, Fine-tuning 손실 함수(finetuning_loss): 0.009741\n",
      "반복(Epoch): 189, Fine-tuning 손실 함수(finetuning_loss): 0.012912\n",
      "반복(Epoch): 190, Fine-tuning 손실 함수(finetuning_loss): 0.018231\n",
      "반복(Epoch): 191, Fine-tuning 손실 함수(finetuning_loss): 0.014695\n",
      "반복(Epoch): 192, Fine-tuning 손실 함수(finetuning_loss): 0.015808\n",
      "반복(Epoch): 193, Fine-tuning 손실 함수(finetuning_loss): 0.016287\n",
      "반복(Epoch): 194, Fine-tuning 손실 함수(finetuning_loss): 0.020726\n",
      "반복(Epoch): 195, Fine-tuning 손실 함수(finetuning_loss): 0.010057\n",
      "반복(Epoch): 196, Fine-tuning 손실 함수(finetuning_loss): 0.019900\n",
      "반복(Epoch): 197, Fine-tuning 손실 함수(finetuning_loss): 0.012767\n",
      "반복(Epoch): 198, Fine-tuning 손실 함수(finetuning_loss): 0.011957\n",
      "반복(Epoch): 199, Fine-tuning 손실 함수(finetuning_loss): 0.015869\n",
      "반복(Epoch): 200, Fine-tuning 손실 함수(finetuning_loss): 0.012006\n",
      "Step 2 : MNIST 데이터 분류를 위한 오토인코더 + softmax 분류기 최적화 완료\n",
      "정확도(오토인코더 + softmax 분류기): 0.964400\n"
     ]
    }
   ],
   "source": [
    "# 세션을 열고 그래프 실행 \n",
    "with tf.Session() as sess : \n",
    "    # 변수들의 초기값 할당\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 전체 배치 개수 불러옴\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    # Step 1 : MNIST 데이터 재구축을 위한 오토인코더 최적화 (pre-training)\n",
    "    for epoch in range(num_epochs): \n",
    "        # 모든 배치들에 대해서 최적화 수행\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, pretraining_loss_print = sess.run([pretraining_train_step, pretraining_loss],feed_dict={x:batch_xs})\n",
    "            \n",
    "        # 지정된 epoch마다 학습결과 출력\n",
    "        if epoch % display_step == 0 : \n",
    "            print(\"반복(Epoch): %d, Pre-training 손실 함수(pretraining_loss): %f\" % ((epoch + 1),pretraining_loss_print))\n",
    "    print(\"Step 1 : MNIST 데이터 재구축을 위한 오토인코더 최적화 완료\")\n",
    "    \n",
    "    # Step 2 : MNIST 데이터 분류를 위한 오토인코더 + softmax 분류기 최적화 (Fine-tuning)\n",
    "    for epoch in range(num_epochs  + 100) : \n",
    "        # 모든 배치들에 대해서 최적화 수행\n",
    "        for i in range(total_batch) : \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, finetuning_loss_print = sess.run([finetuning_train_step,\n",
    "            finetuning_loss], feed_dict = {x:batch_xs, y : batch_ys})\n",
    "            \n",
    "        # 지정된 epoch마다 학습결과 출력\n",
    "        if epoch % display_step == 0 : \n",
    "             print(\"반복(Epoch): %d, Fine-tuning 손실 함수(finetuning_loss): %f\" % ((epoch + 1), finetuning_loss_print))\n",
    "    print(\"Step 2 : MNIST 데이터 분류를 위한 오토인코더 + softmax 분류기 최적화 완료\")\n",
    "    \n",
    "    # 오토인코더 + softmax 분류기 모델의 정확도 출력\n",
    "    correct_prediction = tf.equal = tf.equal(tf.argmax(y,1), tf.argmax(y_pred_softmax,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"정확도(오토인코더 + softmax 분류기): %f\" % sess.run(accuracy, feed_dict = {x:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]   # x1 x2 \n",
    "y_data = [[0],[0],[0],[1],[1],[1]]  # y는 binary data\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32,shape=[None,2]) # x data는 변수 개수가 2개라서 2, 데이터 갯수는 더 많은 것을 줄 수 있으므로 n개 -> none\n",
    "Y = tf.placeholder(tf.float32,shape=[None,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([2,1]),name='weight') # W의 shape 주의 ! 행렬 곱셈할 때 \n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias') # y가 하나라서 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X,W)+b))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b) \n",
    "\n",
    "# cost/loss function \n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuarcy computation    # 일반적으로, 0.5 이상이면 -> pass.1\n",
    "# True if hypothesis > 0.5 else False   0.5 이상이면 true라고 나오고, 아니면 false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)  \n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.7398759\n",
      "200 0.55723894\n",
      "400 0.5295874\n",
      "600 0.5049425\n",
      "800 0.4828868\n",
      "1000 0.46302107\n",
      "1200 0.44497165\n",
      "1400 0.4284183\n",
      "1600 0.41310394\n",
      "1800 0.39883044\n",
      "2000 0.38544998\n",
      "2200 0.37285173\n",
      "2400 0.360953\n",
      "2600 0.34968925\n",
      "2800 0.33900955\n",
      "3000 0.32887137\n",
      "3200 0.31923825\n",
      "3400 0.31007782\n",
      "3600 0.30136117\n",
      "3800 0.29306155\n",
      "4000 0.28515434\n",
      "4200 0.27761647\n",
      "4400 0.27042654\n",
      "4600 0.26356435\n",
      "4800 0.2570111\n",
      "5000 0.25074902\n",
      "5200 0.24476163\n",
      "5400 0.2390333\n",
      "5600 0.23354937\n",
      "5800 0.22829624\n",
      "6000 0.223261\n",
      "6200 0.21843173\n",
      "6400 0.21379717\n",
      "6600 0.20934664\n",
      "6800 0.20507038\n",
      "7000 0.20095907\n",
      "7200 0.1970041\n",
      "7400 0.19319707\n",
      "7600 0.18953077\n",
      "7800 0.18599768\n",
      "8000 0.18259126\n",
      "8200 0.17930514\n",
      "8400 0.17613338\n",
      "8600 0.17307036\n",
      "8800 0.17011088\n",
      "9000 0.16725008\n",
      "9200 0.16448316\n",
      "9400 0.16180585\n",
      "9600 0.15921395\n",
      "9800 0.15670364\n",
      "10000 0.15427129\n",
      "\n",
      "Hypothesis:  [[0.03277376]\n",
      " [0.16151117]\n",
      " [0.31446138]\n",
      " [0.77703404]\n",
      " [0.9368027 ]\n",
      " [0.9792436 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# launch graph \n",
    "with tf.Session() as sess : \n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train],feed_dict = {X : x_data, Y: y_data})\n",
    "        if step % 200 == 0 : \n",
    "            print(step, cost_val)\n",
    "            \n",
    "    # Accuracy report\n",
    "    h,c,a = sess.run([hypothesis,predicted,accuracy],\n",
    "                    feed_dict={X : x_data, Y: y_data})\n",
    "    \n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifying diabetes (당뇨병 예측) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',',dtype=np.float32)\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,shape=[None,8])\n",
    "Y = tf.placeholder(tf.float32,shape=[None,1]) \n",
    "W = tf.Variable(tf.random_normal([8,1]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X,W)+b))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b) \n",
    "\n",
    "# cost/loss function \n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)  \n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6833772\n",
      "200 0.6259838\n",
      "400 0.6041183\n",
      "600 0.5924949\n",
      "800 0.58374\n",
      "1000 0.57609504\n",
      "1200 0.569145\n",
      "1400 0.5627647\n",
      "1600 0.55689096\n",
      "1800 0.55147654\n",
      "2000 0.5464804\n",
      "2200 0.5418658\n",
      "2400 0.5375993\n",
      "2600 0.5336508\n",
      "2800 0.5299927\n",
      "3000 0.52660036\n",
      "3200 0.5234511\n",
      "3400 0.5205246\n",
      "3600 0.5178024\n",
      "3800 0.5152678\n",
      "4000 0.5129055\n",
      "4200 0.5107016\n",
      "4400 0.50864357\n",
      "4600 0.5067199\n",
      "4800 0.5049205\n",
      "5000 0.5032355\n",
      "5200 0.5016563\n",
      "5400 0.50017524\n",
      "5600 0.4987848\n",
      "5800 0.49747854\n",
      "6000 0.49625033\n",
      "6200 0.49509463\n",
      "6400 0.49400616\n",
      "6600 0.49298057\n",
      "6800 0.49201345\n",
      "7000 0.4911007\n",
      "7200 0.49023876\n",
      "7400 0.48942414\n",
      "7600 0.48865393\n",
      "7800 0.48792517\n",
      "8000 0.48723525\n",
      "8200 0.48658156\n",
      "8400 0.485962\n",
      "8600 0.48537436\n",
      "8800 0.48481676\n",
      "9000 0.48428717\n",
      "9200 0.48378426\n",
      "9400 0.48330608\n",
      "9600 0.4828514\n",
      "9800 0.4824187\n",
      "10000 0.48200697\n",
      "\n",
      "Hypothesis:  [[0.39951402]\n",
      " [0.9234819 ]\n",
      " [0.27582774]\n",
      " [0.9404812 ]\n",
      " [0.14241701]\n",
      " [0.7697903 ]\n",
      " [0.94158566]\n",
      " [0.60512674]\n",
      " [0.2705074 ]\n",
      " [0.48797262]\n",
      " [0.663087  ]\n",
      " [0.17896378]\n",
      " [0.22553965]\n",
      " [0.29973328]\n",
      " [0.73116267]\n",
      " [0.40205222]\n",
      " [0.7286526 ]\n",
      " [0.8426069 ]\n",
      " [0.8068477 ]\n",
      " [0.5447467 ]\n",
      " [0.6121602 ]\n",
      " [0.1051814 ]\n",
      " [0.681888  ]\n",
      " [0.71073514]\n",
      " [0.34186727]\n",
      " [0.944989  ]\n",
      " [0.63021094]\n",
      " [0.6034024 ]\n",
      " [0.654333  ]\n",
      " [0.45277396]\n",
      " [0.96138275]\n",
      " [0.87604046]\n",
      " [0.6011933 ]\n",
      " [0.8280478 ]\n",
      " [0.37567487]\n",
      " [0.6143402 ]\n",
      " [0.7877202 ]\n",
      " [0.44352344]\n",
      " [0.5261445 ]\n",
      " [0.32015622]\n",
      " [0.8435091 ]\n",
      " [0.13496807]\n",
      " [0.4430502 ]\n",
      " [0.05381867]\n",
      " [0.58692586]\n",
      " [0.92698675]\n",
      " [0.69238746]\n",
      " [0.711726  ]\n",
      " [0.9498716 ]\n",
      " [0.94406414]\n",
      " [0.9406784 ]\n",
      " [0.23952389]\n",
      " [0.35047692]\n",
      " [0.9655508 ]\n",
      " [0.232007  ]\n",
      " [0.37999314]\n",
      " [0.09561998]\n",
      " [0.70778394]\n",
      " [0.8370613 ]\n",
      " [0.5030442 ]\n",
      " [0.93735117]\n",
      " [0.7481853 ]\n",
      " [0.64502776]\n",
      " [0.8568375 ]\n",
      " [0.57066625]\n",
      " [0.45206735]\n",
      " [0.9659927 ]\n",
      " [0.7363716 ]\n",
      " [0.8492129 ]\n",
      " [0.7099561 ]\n",
      " [0.22701922]\n",
      " [0.7371696 ]\n",
      " [0.90876645]\n",
      " [0.9398751 ]\n",
      " [0.84763074]\n",
      " [0.770015  ]\n",
      " [0.40507334]\n",
      " [0.8840513 ]\n",
      " [0.92498314]\n",
      " [0.90804213]\n",
      " [0.8600687 ]\n",
      " [0.8550924 ]\n",
      " [0.2934551 ]\n",
      " [0.8083197 ]\n",
      " [0.5072658 ]\n",
      " [0.8647249 ]\n",
      " [0.45475677]\n",
      " [0.9106601 ]\n",
      " [0.93678737]\n",
      " [0.7839848 ]\n",
      " [0.75313765]\n",
      " [0.6584547 ]\n",
      " [0.7354128 ]\n",
      " [0.6037363 ]\n",
      " [0.9047971 ]\n",
      " [0.9805378 ]\n",
      " [0.906083  ]\n",
      " [0.50651777]\n",
      " [0.21227518]\n",
      " [0.6912487 ]\n",
      " [0.62643933]\n",
      " [0.96241033]\n",
      " [0.70386916]\n",
      " [0.7562568 ]\n",
      " [0.87474453]\n",
      " [0.7276168 ]\n",
      " [0.9199126 ]\n",
      " [0.82057434]\n",
      " [0.5506892 ]\n",
      " [0.37018204]\n",
      " [0.9308513 ]\n",
      " [0.8528671 ]\n",
      " [0.47850922]\n",
      " [0.39718795]\n",
      " [0.6143013 ]\n",
      " [0.8153883 ]\n",
      " [0.87787676]\n",
      " [0.9306276 ]\n",
      " [0.11837894]\n",
      " [0.73002446]\n",
      " [0.85260177]\n",
      " [0.60435945]\n",
      " [0.63099796]\n",
      " [0.74851716]\n",
      " [0.6737852 ]\n",
      " [0.8476447 ]\n",
      " [0.79869235]\n",
      " [0.56650627]\n",
      " [0.5840472 ]\n",
      " [0.39018595]\n",
      " [0.4659431 ]\n",
      " [0.7406799 ]\n",
      " [0.9447869 ]\n",
      " [0.84913695]\n",
      " [0.7851207 ]\n",
      " [0.869745  ]\n",
      " [0.44968954]\n",
      " [0.794845  ]\n",
      " [0.74045146]\n",
      " [0.6935482 ]\n",
      " [0.8920386 ]\n",
      " [0.6240429 ]\n",
      " [0.61831385]\n",
      " [0.6792965 ]\n",
      " [0.8930708 ]\n",
      " [0.7150384 ]\n",
      " [0.4212456 ]\n",
      " [0.9385272 ]\n",
      " [0.61598337]\n",
      " [0.7916238 ]\n",
      " [0.26133025]\n",
      " [0.35661668]\n",
      " [0.10617745]\n",
      " [0.20296872]\n",
      " [0.9173848 ]\n",
      " [0.88892007]\n",
      " [0.94663167]\n",
      " [0.11648497]\n",
      " [0.5382042 ]\n",
      " [0.75264096]\n",
      " [0.5863648 ]\n",
      " [0.85869944]\n",
      " [0.40589744]\n",
      " [0.8041152 ]\n",
      " [0.65732   ]\n",
      " [0.6253536 ]\n",
      " [0.71107376]\n",
      " [0.87546015]\n",
      " [0.75217205]\n",
      " [0.6426899 ]\n",
      " [0.8827952 ]\n",
      " [0.8429847 ]\n",
      " [0.9490658 ]\n",
      " [0.25293943]\n",
      " [0.786708  ]\n",
      " [0.16309631]\n",
      " [0.33613807]\n",
      " [0.3370765 ]\n",
      " [0.88426024]\n",
      " [0.69324064]\n",
      " [0.92235875]\n",
      " [0.9051474 ]\n",
      " [0.60886425]\n",
      " [0.15932208]\n",
      " [0.20876172]\n",
      " [0.55346483]\n",
      " [0.7454375 ]\n",
      " [0.66060215]\n",
      " [0.83605015]\n",
      " [0.6236202 ]\n",
      " [0.38763514]\n",
      " [0.17054865]\n",
      " [0.9133108 ]\n",
      " [0.38413805]\n",
      " [0.8621615 ]\n",
      " [0.9179786 ]\n",
      " [0.6959429 ]\n",
      " [0.69251776]\n",
      " [0.605022  ]\n",
      " [0.5313885 ]\n",
      " [0.72836447]\n",
      " [0.9575491 ]\n",
      " [0.7333536 ]\n",
      " [0.8349268 ]\n",
      " [0.14240894]\n",
      " [0.28749514]\n",
      " [0.8954146 ]\n",
      " [0.22897586]\n",
      " [0.93683624]\n",
      " [0.25785857]\n",
      " [0.2646262 ]\n",
      " [0.4881495 ]\n",
      " [0.7090742 ]\n",
      " [0.20435116]\n",
      " [0.7470641 ]\n",
      " [0.7439509 ]\n",
      " [0.7839177 ]\n",
      " [0.63214445]\n",
      " [0.15549624]\n",
      " [0.30153358]\n",
      " [0.7322037 ]\n",
      " [0.5325487 ]\n",
      " [0.933399  ]\n",
      " [0.9345392 ]\n",
      " [0.67966014]\n",
      " [0.3871519 ]\n",
      " [0.03165501]\n",
      " [0.64585567]\n",
      " [0.31232944]\n",
      " [0.40202224]\n",
      " [0.94991523]\n",
      " [0.61984485]\n",
      " [0.9495312 ]\n",
      " [0.21690619]\n",
      " [0.14270294]\n",
      " [0.31142917]\n",
      " [0.7819425 ]\n",
      " [0.9074683 ]\n",
      " [0.87952554]\n",
      " [0.689723  ]\n",
      " [0.6836207 ]\n",
      " [0.5899012 ]\n",
      " [0.16346285]\n",
      " [0.55806935]\n",
      " [0.11662301]\n",
      " [0.5848589 ]\n",
      " [0.87468517]\n",
      " [0.6662669 ]\n",
      " [0.7083921 ]\n",
      " [0.95647436]\n",
      " [0.80569226]\n",
      " [0.7817373 ]\n",
      " [0.7300458 ]\n",
      " [0.7689395 ]\n",
      " [0.87579656]\n",
      " [0.49396497]\n",
      " [0.49806723]\n",
      " [0.52469385]\n",
      " [0.80717456]\n",
      " [0.65092397]\n",
      " [0.6842036 ]\n",
      " [0.788473  ]\n",
      " [0.3158264 ]\n",
      " [0.44248176]\n",
      " [0.57683647]\n",
      " [0.65985155]\n",
      " [0.3267805 ]\n",
      " [0.91034245]\n",
      " [0.7719595 ]\n",
      " [0.90333533]\n",
      " [0.53167754]\n",
      " [0.73022825]\n",
      " [0.8399621 ]\n",
      " [0.84838957]\n",
      " [0.6446303 ]\n",
      " [0.8781135 ]\n",
      " [0.35624504]\n",
      " [0.59998924]\n",
      " [0.70783484]\n",
      " [0.37473023]\n",
      " [0.7894318 ]\n",
      " [0.26993155]\n",
      " [0.53024346]\n",
      " [0.9515202 ]\n",
      " [0.7724855 ]\n",
      " [0.8187525 ]\n",
      " [0.6924355 ]\n",
      " [0.41748807]\n",
      " [0.6346336 ]\n",
      " [0.457848  ]\n",
      " [0.48960415]\n",
      " [0.67621243]\n",
      " [0.65197825]\n",
      " [0.6352778 ]\n",
      " [0.59507674]\n",
      " [0.2269747 ]\n",
      " [0.7160784 ]\n",
      " [0.89077693]\n",
      " [0.39807358]\n",
      " [0.67068946]\n",
      " [0.7484633 ]\n",
      " [0.54996496]\n",
      " [0.77313626]\n",
      " [0.5205295 ]\n",
      " [0.7003311 ]\n",
      " [0.8969883 ]\n",
      " [0.6468414 ]\n",
      " [0.7514573 ]\n",
      " [0.8824029 ]\n",
      " [0.4917573 ]\n",
      " [0.86212647]\n",
      " [0.9537723 ]\n",
      " [0.34014496]\n",
      " [0.78217816]\n",
      " [0.29344368]\n",
      " [0.8094794 ]\n",
      " [0.8173387 ]\n",
      " [0.73611194]\n",
      " [0.39593467]\n",
      " [0.7618768 ]\n",
      " [0.7978713 ]\n",
      " [0.73814344]\n",
      " [0.2093966 ]\n",
      " [0.78312796]\n",
      " [0.8584398 ]\n",
      " [0.5670073 ]\n",
      " [0.9466421 ]\n",
      " [0.2477093 ]\n",
      " [0.71114695]\n",
      " [0.95824623]\n",
      " [0.21568206]\n",
      " [0.43628097]\n",
      " [0.6624933 ]\n",
      " [0.34034026]\n",
      " [0.17456308]\n",
      " [0.8734563 ]\n",
      " [0.916225  ]\n",
      " [0.8527274 ]\n",
      " [0.5936146 ]\n",
      " [0.6007699 ]\n",
      " [0.54397225]\n",
      " [0.81102395]\n",
      " [0.8366083 ]\n",
      " [0.9474234 ]\n",
      " [0.6959335 ]\n",
      " [0.7125713 ]\n",
      " [0.5870801 ]\n",
      " [0.91641474]\n",
      " [0.9463035 ]\n",
      " [0.6374389 ]\n",
      " [0.28303084]\n",
      " [0.6842829 ]\n",
      " [0.34995043]\n",
      " [0.73539066]\n",
      " [0.21874541]\n",
      " [0.30364633]\n",
      " [0.42497128]\n",
      " [0.60253227]\n",
      " [0.3174631 ]\n",
      " [0.59105414]\n",
      " [0.8471945 ]\n",
      " [0.66419613]\n",
      " [0.87257314]\n",
      " [0.94992733]\n",
      " [0.7401273 ]\n",
      " [0.07282528]\n",
      " [0.42742738]\n",
      " [0.82472813]\n",
      " [0.86066103]\n",
      " [0.66516435]\n",
      " [0.25435406]\n",
      " [0.9019083 ]\n",
      " [0.8854113 ]\n",
      " [0.26603162]\n",
      " [0.52394485]\n",
      " [0.8370203 ]\n",
      " [0.8818599 ]\n",
      " [0.868736  ]\n",
      " [0.9035679 ]\n",
      " [0.8972558 ]\n",
      " [0.9418584 ]\n",
      " [0.67097527]\n",
      " [0.5416364 ]\n",
      " [0.54741555]\n",
      " [0.8308054 ]\n",
      " [0.868152  ]\n",
      " [0.2203562 ]\n",
      " [0.82967854]\n",
      " [0.8964019 ]\n",
      " [0.32348326]\n",
      " [0.6206082 ]\n",
      " [0.8643383 ]\n",
      " [0.58166176]\n",
      " [0.9135996 ]\n",
      " [0.3250279 ]\n",
      " [0.8245027 ]\n",
      " [0.62195814]\n",
      " [0.86731887]\n",
      " [0.38852283]\n",
      " [0.6931591 ]\n",
      " [0.7033541 ]\n",
      " [0.8147451 ]\n",
      " [0.11476195]\n",
      " [0.20744449]\n",
      " [0.6393342 ]\n",
      " [0.813819  ]\n",
      " [0.4228132 ]\n",
      " [0.8090045 ]\n",
      " [0.49783918]\n",
      " [0.39195478]\n",
      " [0.84125876]\n",
      " [0.43566966]\n",
      " [0.9200271 ]\n",
      " [0.8473781 ]\n",
      " [0.6087053 ]\n",
      " [0.91664743]\n",
      " [0.6175412 ]\n",
      " [0.7922596 ]\n",
      " [0.32687092]\n",
      " [0.3101136 ]\n",
      " [0.7483437 ]\n",
      " [0.47566652]\n",
      " [0.44846147]\n",
      " [0.88217497]\n",
      " [0.9074458 ]\n",
      " [0.9081206 ]\n",
      " [0.9504219 ]\n",
      " [0.7143955 ]\n",
      " [0.88653636]\n",
      " [0.36529297]\n",
      " [0.36702186]\n",
      " [0.49886382]\n",
      " [0.9475018 ]\n",
      " [0.53944135]\n",
      " [0.18852079]\n",
      " [0.92858934]\n",
      " [0.83218575]\n",
      " [0.54995257]\n",
      " [0.8256073 ]\n",
      " [0.0127725 ]\n",
      " [0.9266535 ]\n",
      " [0.7553607 ]\n",
      " [0.7658049 ]\n",
      " [0.79925823]\n",
      " [0.96999776]\n",
      " [0.6440072 ]\n",
      " [0.76146126]\n",
      " [0.7103042 ]\n",
      " [0.85204583]\n",
      " [0.2435982 ]\n",
      " [0.6081269 ]\n",
      " [0.9124067 ]\n",
      " [0.6054871 ]\n",
      " [0.77825177]\n",
      " [0.9413103 ]\n",
      " [0.81791484]\n",
      " [0.87863314]\n",
      " [0.4958805 ]\n",
      " [0.8272588 ]\n",
      " [0.9529848 ]\n",
      " [0.74195296]\n",
      " [0.6566828 ]\n",
      " [0.29671228]\n",
      " [0.4319672 ]\n",
      " [0.56791335]\n",
      " [0.6300345 ]\n",
      " [0.51240295]\n",
      " [0.79104793]\n",
      " [0.588245  ]\n",
      " [0.7505101 ]\n",
      " [0.8555463 ]\n",
      " [0.76942205]\n",
      " [0.6312463 ]\n",
      " [0.5126818 ]\n",
      " [0.58454037]\n",
      " [0.9419217 ]\n",
      " [0.8318613 ]\n",
      " [0.23980698]\n",
      " [0.4384872 ]\n",
      " [0.4546197 ]\n",
      " [0.08948982]\n",
      " [0.8986091 ]\n",
      " [0.16168505]\n",
      " [0.8893274 ]\n",
      " [0.8502568 ]\n",
      " [0.827085  ]\n",
      " [0.6804007 ]\n",
      " [0.87997043]\n",
      " [0.36759728]\n",
      " [0.7836851 ]\n",
      " [0.9387481 ]\n",
      " [0.38375705]\n",
      " [0.46150064]\n",
      " [0.87577164]\n",
      " [0.85545564]\n",
      " [0.5997051 ]\n",
      " [0.8023751 ]\n",
      " [0.7789968 ]\n",
      " [0.7721503 ]\n",
      " [0.31207395]\n",
      " [0.7671432 ]\n",
      " [0.8868693 ]\n",
      " [0.60336465]\n",
      " [0.7853899 ]\n",
      " [0.76687807]\n",
      " [0.80183315]\n",
      " [0.8607621 ]\n",
      " [0.9451066 ]\n",
      " [0.6657492 ]\n",
      " [0.40929538]\n",
      " [0.7610868 ]\n",
      " [0.7772863 ]\n",
      " [0.9680047 ]\n",
      " [0.7726474 ]\n",
      " [0.7012801 ]\n",
      " [0.38942745]\n",
      " [0.731957  ]\n",
      " [0.92535305]\n",
      " [0.9559582 ]\n",
      " [0.9131459 ]\n",
      " [0.7289073 ]\n",
      " [0.673585  ]\n",
      " [0.83017325]\n",
      " [0.41166908]\n",
      " [0.7594045 ]\n",
      " [0.78760886]\n",
      " [0.878155  ]\n",
      " [0.6289493 ]\n",
      " [0.6802431 ]\n",
      " [0.8716108 ]\n",
      " [0.48542988]\n",
      " [0.47895306]\n",
      " [0.60905945]\n",
      " [0.741514  ]\n",
      " [0.59282184]\n",
      " [0.89145184]\n",
      " [0.9172851 ]\n",
      " [0.23378095]\n",
      " [0.12871647]\n",
      " [0.7887879 ]\n",
      " [0.5230554 ]\n",
      " [0.25142574]\n",
      " [0.83879495]\n",
      " [0.8986518 ]\n",
      " [0.6575419 ]\n",
      " [0.93852794]\n",
      " [0.90774214]\n",
      " [0.7919268 ]\n",
      " [0.8150003 ]\n",
      " [0.67428637]\n",
      " [0.51493305]\n",
      " [0.7507821 ]\n",
      " [0.5805299 ]\n",
      " [0.14619541]\n",
      " [0.89132386]\n",
      " [0.89555144]\n",
      " [0.6867219 ]\n",
      " [0.9278408 ]\n",
      " [0.8370898 ]\n",
      " [0.8821089 ]\n",
      " [0.6230027 ]\n",
      " [0.7142915 ]\n",
      " [0.86166763]\n",
      " [0.71531487]\n",
      " [0.8722162 ]\n",
      " [0.9158391 ]\n",
      " [0.59457946]\n",
      " [0.8137883 ]\n",
      " [0.85518795]\n",
      " [0.45162857]\n",
      " [0.5601944 ]\n",
      " [0.0814015 ]\n",
      " [0.2279056 ]\n",
      " [0.85789514]\n",
      " [0.65876794]\n",
      " [0.63138324]\n",
      " [0.5488491 ]\n",
      " [0.9467615 ]\n",
      " [0.4506567 ]\n",
      " [0.82216465]\n",
      " [0.26657912]\n",
      " [0.90222853]\n",
      " [0.26529408]\n",
      " [0.75432515]\n",
      " [0.5406788 ]\n",
      " [0.84686494]\n",
      " [0.5687274 ]\n",
      " [0.30313504]\n",
      " [0.7146789 ]\n",
      " [0.94306165]\n",
      " [0.38801774]\n",
      " [0.9389363 ]\n",
      " [0.8732405 ]\n",
      " [0.8617171 ]\n",
      " [0.82190156]\n",
      " [0.41320953]\n",
      " [0.37548476]\n",
      " [0.6678741 ]\n",
      " [0.17601907]\n",
      " [0.9576463 ]\n",
      " [0.3497616 ]\n",
      " [0.94127965]\n",
      " [0.8953296 ]\n",
      " [0.47550347]\n",
      " [0.1953209 ]\n",
      " [0.6775069 ]\n",
      " [0.4435365 ]\n",
      " [0.8375581 ]\n",
      " [0.6976509 ]\n",
      " [0.98271334]\n",
      " [0.5147814 ]\n",
      " [0.65566045]\n",
      " [0.7582401 ]\n",
      " [0.77701104]\n",
      " [0.05929661]\n",
      " [0.70828015]\n",
      " [0.78696793]\n",
      " [0.8124429 ]\n",
      " [0.6488416 ]\n",
      " [0.44617927]\n",
      " [0.5775594 ]\n",
      " [0.90676343]\n",
      " [0.6311419 ]\n",
      " [0.778676  ]\n",
      " [0.82566226]\n",
      " [0.88091433]\n",
      " [0.8126658 ]\n",
      " [0.5602707 ]\n",
      " [0.77473617]\n",
      " [0.89875066]\n",
      " [0.6583835 ]\n",
      " [0.9655584 ]\n",
      " [0.80466366]\n",
      " [0.6194828 ]\n",
      " [0.49255842]\n",
      " [0.81784093]\n",
      " [0.84997535]\n",
      " [0.4879472 ]\n",
      " [0.72312593]\n",
      " [0.2972228 ]\n",
      " [0.59734994]\n",
      " [0.8444846 ]\n",
      " [0.95504814]\n",
      " [0.84324086]\n",
      " [0.7363594 ]\n",
      " [0.76882124]\n",
      " [0.89054453]\n",
      " [0.5349184 ]\n",
      " [0.9435569 ]\n",
      " [0.49095544]\n",
      " [0.8081664 ]\n",
      " [0.33327043]\n",
      " [0.08212909]\n",
      " [0.27350572]\n",
      " [0.33703634]\n",
      " [0.6981939 ]\n",
      " [0.8173141 ]\n",
      " [0.5779101 ]\n",
      " [0.76461285]\n",
      " [0.8026796 ]\n",
      " [0.5452558 ]\n",
      " [0.4028813 ]\n",
      " [0.9029178 ]\n",
      " [0.8648013 ]\n",
      " [0.30082214]\n",
      " [0.5837045 ]\n",
      " [0.22566488]\n",
      " [0.41365343]\n",
      " [0.7344856 ]\n",
      " [0.71318173]\n",
      " [0.9144409 ]\n",
      " [0.9784035 ]\n",
      " [0.18755475]\n",
      " [0.70197284]\n",
      " [0.5933101 ]\n",
      " [0.38354945]\n",
      " [0.73127604]\n",
      " [0.7651564 ]\n",
      " [0.9069805 ]\n",
      " [0.7575002 ]\n",
      " [0.43165296]\n",
      " [0.6397539 ]\n",
      " [0.13916525]\n",
      " [0.6593977 ]\n",
      " [0.5100323 ]\n",
      " [0.9129296 ]\n",
      " [0.5878201 ]\n",
      " [0.6341822 ]\n",
      " [0.81212187]\n",
      " [0.72995603]\n",
      " [0.40358037]\n",
      " [0.7490579 ]\n",
      " [0.6431912 ]\n",
      " [0.30277216]\n",
      " [0.5792812 ]\n",
      " [0.89862585]\n",
      " [0.81060004]\n",
      " [0.5987799 ]\n",
      " [0.8175384 ]\n",
      " [0.30858046]\n",
      " [0.8291606 ]\n",
      " [0.6609037 ]\n",
      " [0.7682704 ]\n",
      " [0.40540805]\n",
      " [0.6978447 ]\n",
      " [0.8268262 ]\n",
      " [0.20746186]\n",
      " [0.30703104]\n",
      " [0.8088853 ]\n",
      " [0.80764836]\n",
      " [0.7595841 ]\n",
      " [0.8995561 ]\n",
      " [0.76030093]\n",
      " [0.7233598 ]\n",
      " [0.7049277 ]\n",
      " [0.7257542 ]\n",
      " [0.66007173]\n",
      " [0.7836081 ]\n",
      " [0.52227426]\n",
      " [0.5003836 ]\n",
      " [0.88609767]\n",
      " [0.8159015 ]\n",
      " [0.6547432 ]\n",
      " [0.27694303]\n",
      " [0.89199626]\n",
      " [0.7939569 ]\n",
      " [0.8288319 ]\n",
      " [0.67429066]\n",
      " [0.8651695 ]\n",
      " [0.8535707 ]\n",
      " [0.71982133]\n",
      " [0.36941117]\n",
      " [0.9071989 ]\n",
      " [0.92554843]\n",
      " [0.30283698]\n",
      " [0.1479508 ]\n",
      " [0.71763694]\n",
      " [0.34422302]\n",
      " [0.7211023 ]\n",
      " [0.37180492]\n",
      " [0.49813566]\n",
      " [0.41936105]\n",
      " [0.7577981 ]\n",
      " [0.8862448 ]\n",
      " [0.14780042]\n",
      " [0.39219183]\n",
      " [0.5533955 ]\n",
      " [0.4913067 ]\n",
      " [0.48582128]\n",
      " [0.7598707 ]\n",
      " [0.15242976]\n",
      " [0.91985965]\n",
      " [0.18713425]\n",
      " [0.8606416 ]\n",
      " [0.6900575 ]\n",
      " [0.72557503]\n",
      " [0.8546576 ]\n",
      " [0.7029364 ]\n",
      " [0.8927242 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.770751\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess : \n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    feed = {X : x_data, Y: y_data}\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train],feed_dict = feed)\n",
    "        if step % 200 == 0 : \n",
    "            print(step, cost_val)\n",
    "            \n",
    "    # Accuracy report\n",
    "    h,c,a = sess.run([hypothesis,predicted,accuracy],\n",
    "                    feed_dict=feed)\n",
    "    \n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 캐글 데이터 다운받아서 해보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab6 \n",
    "# Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여러 개의 클래스를 예측할 때 유용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]]\n",
    "y_data = [[0,0,1],[0,0,1],[0,0,1],[0,1,0],[0,1,0],[0,1,0],[1,0,0],[1,0,0]] # one-hot encoding : 하나만 핫하게 한다? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\",[None,4])\n",
    "Y = tf.placeholder(\"float\",[None,3]) # one-hot 으로 표현할 때는 label의 갯수가 y의 값이 됨 class의 갯수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4,nb_classes]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]),name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis),axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.3397064\n",
      "200 0.6502036\n",
      "400 0.55146444\n",
      "600 0.46264607\n",
      "800 0.37525833\n",
      "1000 0.28672743\n",
      "1200 0.22175817\n",
      "1400 0.20189479\n",
      "1600 0.18525277\n",
      "1800 0.1710692\n",
      "2000 0.15884331\n"
     ]
    }
   ],
   "source": [
    "# Launch graph \n",
    "with tf.Session() as sess : \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001): \n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step%200 == 0 : \n",
    "            print(step, sess.run(cost, feed_dict={X :x_data, Y : y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4150488e-03 2.5159691e-04 9.9533337e-01]] [2]\n"
     ]
    }
   ],
   "source": [
    "# Testing & One-hot encoding\n",
    "with tf.Session() as sess : \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    a = sess.run(hypothesis, feed_dict={X : [[1,11,7,9]]})\n",
    "    print(a, sess.run(tf.arg_max(a,1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fancy softmax classifier\n",
    "- cross_entropy\n",
    "- one_hot\n",
    "- reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting animal type based on various features 동물 특징들 통한 동물 분류\n",
    "xy = np.loadtxt('data-04-zoo.csv',delimiter=',',dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 7\n",
    "Y = tf.placeholder(tf.int32, [None,1]) #0~6, shape(?,1)\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes) #one hot shape=(?,1,7) , one hot을 해버리면 한 차원 더 더해버림 \n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1,nb_classes]) # shape=(?,7) <-그래서 이렇게 바꿔줘야 함  (nb_classes : 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([16,nb_classes]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]),name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(X,W) + b\n",
    "hypothesis = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_cross_entropy_with_logits 함수 \n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot) # 로짓 : 소프트맥스 전의 함수, 레이블 : y 레이블 \n",
    "cost = tf.reduce_mean(cost_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.argmax(hypothesis,1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot,1))     # prediction 과 tf.arg~값을 비교 \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # 맞게 예측한 것을 모아서 평균을 낸다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tLoss: 3.391\tAcc: 56.44%\n",
      "Step:   100\tLoss: 0.679\tAcc: 77.23%\n",
      "Step:   200\tLoss: 0.451\tAcc: 87.13%\n",
      "Step:   300\tLoss: 0.332\tAcc: 90.10%\n",
      "Step:   400\tLoss: 0.263\tAcc: 94.06%\n",
      "Step:   500\tLoss: 0.218\tAcc: 95.05%\n",
      "Step:   600\tLoss: 0.187\tAcc: 95.05%\n",
      "Step:   700\tLoss: 0.164\tAcc: 95.05%\n",
      "Step:   800\tLoss: 0.146\tAcc: 95.05%\n",
      "Step:   900\tLoss: 0.131\tAcc: 95.05%\n",
      "Step:  1000\tLoss: 0.118\tAcc: 97.03%\n",
      "Step:  1100\tLoss: 0.108\tAcc: 97.03%\n",
      "Step:  1200\tLoss: 0.099\tAcc: 98.02%\n",
      "Step:  1300\tLoss: 0.091\tAcc: 99.01%\n",
      "Step:  1400\tLoss: 0.085\tAcc: 100.00%\n",
      "Step:  1500\tLoss: 0.079\tAcc: 100.00%\n",
      "Step:  1600\tLoss: 0.074\tAcc: 100.00%\n",
      "Step:  1700\tLoss: 0.069\tAcc: 100.00%\n",
      "Step:  1800\tLoss: 0.065\tAcc: 100.00%\n",
      "Step:  1900\tLoss: 0.062\tAcc: 100.00%\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 6 True Y : 6\n",
      "[True] Prediction: 6 True Y : 6\n",
      "[True] Prediction: 6 True Y : 6\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 5 True Y : 5\n",
      "[True] Prediction: 4 True Y : 4\n",
      "[True] Prediction: 4 True Y : 4\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 5 True Y : 5\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 5 True Y : 5\n",
      "[True] Prediction: 5 True Y : 5\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 5 True Y : 5\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 6 True Y : 6\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 5 True Y : 5\n",
      "[True] Prediction: 4 True Y : 4\n",
      "[True] Prediction: 6 True Y : 6\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 2 True Y : 2\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 6 True Y : 6\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 2 True Y : 2\n",
      "[True] Prediction: 6 True Y : 6\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 2 True Y : 2\n",
      "[True] Prediction: 6 True Y : 6\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 6 True Y : 6\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 5 True Y : 5\n",
      "[True] Prediction: 4 True Y : 4\n",
      "[True] Prediction: 2 True Y : 2\n",
      "[True] Prediction: 2 True Y : 2\n",
      "[True] Prediction: 3 True Y : 3\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 1 True Y : 1\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 5 True Y : 5\n",
      "[True] Prediction: 0 True Y : 0\n",
      "[True] Prediction: 6 True Y : 6\n",
      "[True] Prediction: 1 True Y : 1\n"
     ]
    }
   ],
   "source": [
    "# Launch graph \n",
    "with tf.Session() as sess : \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer, feed_dict={X : x_data, Y: y_data})\n",
    "        if step % 100 == 0 : \n",
    "            loss, acc = sess.run([cost, accuracy],feed_dict={X : x_data, Y : y_data})\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step,loss,acc))\n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "    # y_data : (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p,y in zip(pred, y_data.flatten()): # flatten? [[1],[0]] => [1,0]  \n",
    "        print(\"[{}] Prediction: {} True Y : {}\".format(p==int(y),p, int(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
